"""
Hallucination Checker module for RAG applications.
This module provides functionality to evaluate the faithfulness of responses
generated by the Research Expert agent using OpenAI directly.
"""

import os
from typing import List, Dict, Any, Tuple
import re
from dotenv import load_dotenv
import json

# Load environment variables
load_dotenv()

# OpenAI imports
from openai import OpenAI

class HallucinationChecker:
    """
    A class to check for hallucinations in RAG responses using OpenAI.
    """
    
    def __init__(self):
        """Initialize the HallucinationChecker with necessary models."""
        print("Initializing HallucinationChecker...")
        
        # Initialize OpenAI client
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")
        
        self.client = OpenAI(api_key=openai_api_key)
        print("HallucinationChecker initialized successfully")
    
    def extract_text_from_urls(self, urls: List[str], web_search_results: str) -> List[str]:
        """
        Extract relevant text snippets from web search results for each URL.
        
        Args:
            urls: List of URLs found in the search results
            web_search_results: Raw web search results text
            
        Returns:
            List of text snippets corresponding to each URL
        """
        print(f"Extracting text from {len(urls)} URLs")
        # Simple approach: split the web search results by URL mentions
        contexts = []
        
        # If no URLs, return the entire web search results as context
        if not urls:
            print("No URLs provided, using entire web search results as context")
            return [web_search_results] if web_search_results else []
        
        # For each URL, try to find related content
        for url in urls:
            # Look for content that appears after the URL
            url_pattern = re.escape(url)
            matches = re.split(url_pattern, web_search_results)
            
            if len(matches) > 1:
                # Take the content after the URL, limited to a reasonable size
                context = matches[1].split("http")[0] if "http" in matches[1] else matches[1]
                # Limit context to 500 characters
                context = context[:500].strip()
                if context:
                    contexts.append(context)
                    print(f"Extracted context for {url}: {context[:50]}...")
        
        # If no contexts were extracted, use the whole search results
        if not contexts and web_search_results:
            print("No specific contexts extracted, using entire web search results")
            contexts = [web_search_results]
            
        print(f"Extracted {len(contexts)} context snippets")
        return contexts
    
    def evaluate_response(self, 
                         query: str, 
                         response: str, 
                         web_search_results: str,
                         sources: List[str]) -> Dict[str, Any]:
        """
        Evaluate a response for hallucinations using OpenAI.
        
        Args:
            query: The user's query
            response: The generated response
            web_search_results: Raw web search results
            sources: List of source URLs
            
        Returns:
            Dictionary containing evaluation scores and hallucination assessment
        """
        print(f"Evaluating response for query: {query[:50]}...")
        print(f"Response length: {len(response)} characters")
        print(f"Web search results length: {len(web_search_results)} characters")
        print(f"Number of sources: {len(sources)}")
        
        # Extract contexts from web search results
        contexts = self.extract_text_from_urls(sources, web_search_results)
        
        # If no contexts available, we can't evaluate faithfulness
        if not contexts:
            print("WARNING: No contexts available to evaluate faithfulness")
            return {
                "faithfulness_score": 0.0,
                "answer_relevancy_score": 0.0,
                "has_hallucination": True,
                "confidence": "Low",
                "warning": "No context available to verify response."
            }
        
        try:
            # Combine contexts into a single string
            combined_context = "\n\n".join(contexts)
            
            # Prepare the prompt for OpenAI
            system_message = """
            You are an expert at evaluating AI-generated responses for hallucinations.
            Your task is to evaluate if a response is faithful to the provided context.
            Score the response on two metrics:
            1. Faithfulness (0-1): How well the response is grounded in the provided context
            2. Answer Relevancy (0-1): How well the response addresses the query
            
            Provide your evaluation as a JSON object with the following structure:
            {
                "faithfulness_score": float,
                "answer_relevancy_score": float,
                "has_hallucination": boolean,
                "confidence": "High" | "Medium" | "Low",
                "explanation": string
            }
            """
            
            user_message = f"""
            Query: {query}
            
            Response to evaluate: {response}
            
            Context from sources:
            {combined_context}
            
            Evaluate the response for hallucinations and provide your assessment in the JSON format.
            """
            
            # Call OpenAI API
            print("Calling OpenAI API for hallucination evaluation...")
            completion = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_message}
                ],
                temperature=0,
                response_format={"type": "json_object"}
            )
            
            # Extract and parse the evaluation
            evaluation_text = completion.choices[0].message.content
            evaluation = json.loads(evaluation_text)
            print(f"OpenAI evaluation: {evaluation}")
            
            # Generate appropriate warning message
            if evaluation["has_hallucination"]:
                if evaluation["faithfulness_score"] < 0.3:
                    warning = "⚠️ High risk of hallucination detected. Verify information independently."
                else:
                    warning = "⚠️ Potential hallucination detected. Some information may not be supported by sources."
            else:
                warning = "✅ Response appears to be well-grounded in the provided sources."
            
            # Add warning to the evaluation
            evaluation["warning"] = warning
            
            print(f"Hallucination assessment: {warning}")
            
            return {
                "faithfulness_score": round(evaluation["faithfulness_score"], 2),
                "answer_relevancy_score": round(evaluation["answer_relevancy_score"], 2),
                "has_hallucination": evaluation["has_hallucination"],
                "confidence": evaluation["confidence"],
                "warning": warning
            }
            
        except Exception as e:
            import traceback
            print(f"ERROR in hallucination evaluation: {str(e)}")
            traceback.print_exc()
            # Return default values in case of error
            return {
                "faithfulness_score": 0.0,
                "answer_relevancy_score": 0.0,
                "has_hallucination": True,
                "confidence": "Low",
                "warning": f"Error evaluating response: {str(e)}"
            }
    
    def format_evaluation_results(self, evaluation: Dict[str, Any]) -> str:
        """
        Format evaluation results into a readable string for display.
        
        Args:
            evaluation: Dictionary containing evaluation scores
            
        Returns:
            Formatted string with evaluation results
        """
        print("Formatting evaluation results for display")
        result = "\n\n**Hallucination Check:**\n"
        result += f"{evaluation['warning']}\n"
        result += f"- Faithfulness: {evaluation['faithfulness_score']:.2f}/1.00 ({evaluation['confidence']} confidence)\n"
        result += f"- Answer Relevancy: {evaluation['answer_relevancy_score']:.2f}/1.00\n"
        
        print(f"Formatted results: {result}")
        return result
