"""
Hallucination Checker module for RAG applications.
This module provides functionality to evaluate the faithfulness of responses
generated by the Research Expert agent using OpenAI's evaluation capabilities.
"""

import os
from typing import List, Dict, Any, Optional, Tuple
import re
from dotenv import load_dotenv
import json
import hashlib
import datetime
import urllib.parse
from collections import defaultdict

# Load environment variables
load_dotenv()

# OpenAI imports
from openai import OpenAI

class HallucinationChecker:
    """
    A class to check for hallucinations in RAG responses using OpenAI's evaluation capabilities.
    Implements industry best practices for hallucination detection in AI-generated content.
    """
    
    def __init__(self, model: str = "gpt-4o"):
        """
        Initialize the HallucinationChecker with necessary models.
        
        Args:
            model: The OpenAI model to use for evaluation (default: gpt-4o)
        """
        print("Initializing HallucinationChecker...")
        
        # Initialize OpenAI client
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")
        
        self.client = OpenAI(api_key=openai_api_key)
        self.model = model
        
        # Cache for storing evaluation results to avoid redundant processing
        self.evaluation_cache = {}
        
        # Store credibility scores for domains
        self.domain_credibility = {}
        
        print(f"HallucinationChecker initialized with model: {model}")
    
    def _extract_domain(self, url: str) -> str:
        """
        Extract domain from URL for credibility assessment.
        
        Args:
            url: URL to extract domain from
            
        Returns:
            Domain name
        """
        try:
            parsed_url = urllib.parse.urlparse(url)
            domain = parsed_url.netloc
            return domain
        except:
            return url
    
    def prepare_context(self, web_search_results: str, sources: List[str]) -> Dict[str, Any]:
        """
        Prepare context from web search results and sources for evaluation.
        Enhanced with semantic chunking and better source attribution.
        
        Args:
            web_search_results: Raw web search results text
            sources: List of source URLs
            
        Returns:
            Processed context with metadata for evaluation
        """
        print(f"Preparing context from {len(sources)} sources")
        
        # If no web search results, return empty context
        if not web_search_results:
            print("No web search results provided")
            return {"processed_text": "", "chunks": [], "source_map": {}}
        
        # Create semantic chunks from the web search results
        chunks = []
        source_map = {}  # Maps chunks to sources
        
        # Process each source URL to find related content in web search results
        for url in sources:
            # Clean URL for pattern matching
            clean_url = re.escape(url.strip())
            
            # Try to find content associated with this URL
            url_pattern = re.compile(f"{clean_url}[^h]*?(?=http|$)", re.DOTALL)
            matches = url_pattern.findall(web_search_results)
            
            if matches:
                for match in matches:
                    # Generate a unique ID for this chunk
                    chunk_id = hashlib.md5(match.encode()).hexdigest()[:8]
                    
                    # Create a chunk with metadata
                    chunk = {
                        "id": chunk_id,
                        "text": match.strip(),
                        "source": url,
                        "domain": self._extract_domain(url),
                        "char_length": len(match.strip())
                    }
                    
                    chunks.append(chunk)
                    source_map[chunk_id] = url
        
        # If no specific chunks were found, split the entire web search results
        if not chunks:
            print("Using entire web search results, splitting into chunks")
            # Split content into chunks of approx 1000 chars
            full_text = web_search_results
            chunk_size = 1000
            for i in range(0, len(full_text), chunk_size):
                chunk_text = full_text[i:i+chunk_size]
                chunk_id = f"generic_{i//chunk_size}"
                
                chunk = {
                    "id": chunk_id,
                    "text": chunk_text,
                    "source": "general_search_results",
                    "domain": "multiple_sources",
                    "char_length": len(chunk_text)
                }
                
                chunks.append(chunk)
                source_map[chunk_id] = "multiple_sources"
        
        # Sort chunks by length (longest first) to prioritize more detailed information
        chunks.sort(key=lambda x: x["char_length"], reverse=True)
        
        # Combine chunks into processed text while preserving metadata
        processed_text = ""
        for chunk in chunks:
            processed_text += f"SOURCE: {chunk['source']}\nCONTENT ID: {chunk['id']}\n{chunk['text']}\n\n---\n\n"
        
        # Truncate if too long (OpenAI has context limits)
        max_context_length = 12000  # Conservative limit
        if len(processed_text) > max_context_length:
            print(f"Context too long ({len(processed_text)} chars), truncating to {max_context_length}")
            processed_text = processed_text[:max_context_length] + "..."
        
        return {
            "processed_text": processed_text,
            "chunks": chunks,
            "source_map": source_map
        }
    
    def verify_sources(self, sources: List[str]) -> Dict[str, float]:
        """
        Evaluate the credibility of sources based on domain reputation.
        Caches results to avoid redundant checks.
        
        Args:
            sources: List of source URLs
            
        Returns:
            Dictionary mapping sources to credibility scores (0-1)
        """
        credibility_scores = {}
        
        for url in sources:
            domain = self._extract_domain(url)
            
            # Check if we already have a credibility score for this domain
            if domain in self.domain_credibility:
                credibility_scores[url] = self.domain_credibility[domain]
                continue
            
            # Default credibility is neutral
            default_score = 0.7
            
            # Domains that typically have higher credibility for technical information
            high_credibility_domains = [
                'arxiv.org', 'github.com', 'wikipedia.org', 'docs.python.org', 'developer.mozilla.org',
                'w3.org', 'docs.microsoft.com', 'aws.amazon.com', 'cloud.google.com', 'api.openai.com',
                'docs.docker.com', 'kubernetes.io', 'tensorflow.org', 'pytorch.org', 'cdc.gov', 'nih.gov',
                'nasa.gov', 'education.gov', 'data.gov', 'census.gov', 'europa.eu', 'www.gov.uk',
                'canada.ca', 'australia.gov.au', 'harvard.edu', 'stanford.edu', 'mit.edu'
            ]
            
            # Check for high credibility domains
            for high_domain in high_credibility_domains:
                if high_domain in domain:
                    credibility_scores[url] = 0.9
                    self.domain_credibility[domain] = 0.9
                    break
            else:
                # Set default score if not high credibility
                credibility_scores[url] = default_score
                self.domain_credibility[domain] = default_score
        
        return credibility_scores
    
    def extract_claims(self, response: str) -> List[str]:
        """
        Extract factual claims from the response text for verification.
        
        Args:
            response: The generated response text
            
        Returns:
            List of extracted factual claims
        """
        try:
            # Use LLM to extract claims
            system_prompt = """You are an expert at identifying factual claims in text.
Extract all specific factual claims from the provided text, focusing on statements that:
- Contain specific facts, numbers, dates, or assertions about the world
- Could be verified or disproven with evidence
- Are not purely opinions, perspectives, or subjective statements

Return the claims as a JSON array of strings, with each claim as a separate string."""

            user_prompt = f"""Extract all factual claims from the following text:

TEXT:
{response}

EXTRACTED CLAIMS (return as JSON array):"""

            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0
            )
            
            claims_text = completion.choices[0].message.content
            
            # Extract JSON array from response
            json_match = re.search(r'\[.*\]', claims_text, re.DOTALL)
            if json_match:
                claims = json.loads(json_match.group(0))
            else:
                # Fallback to simple parsing if JSON array not found
                claims = [line.strip() for line in claims_text.split('\n') if line.strip() and not line.startswith('```')]
            
            print(f"Extracted {len(claims)} factual claims from response")
            return claims
            
        except Exception as e:
            print(f"Error extracting claims: {str(e)}")
            # Fallback to simpler method
            sentences = [s.strip() for s in re.split(r'[.!?]', response) if s.strip()]
            return [s for s in sentences if len(s.split()) > 5]  # Only sentences with at least 5 words
    
    def ground_claims(self, claims: List[str], context_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Ground each claim to specific evidence in the context.
        
        Args:
            claims: List of factual claims
            context_data: Context data with processed text and metadata
            
        Returns:
            Dictionary with grounding information for each claim
        """
        if not claims or not context_data or not context_data.get("processed_text"):
            return {"grounded_claims": [], "ungrounded_claims": claims}
        
        context = context_data["processed_text"]
        source_map = context_data["source_map"]
        
        # For each claim, find supporting evidence in context
        grounding_results = {}
        grounded_claims = []
        ungrounded_claims = []
        
        # Group claims in batches to reduce API calls
        batch_size = 3
        claim_batches = [claims[i:i+batch_size] for i in range(0, len(claims), batch_size)]
        
        for batch in claim_batches:
            batch_text = "\n".join([f"Claim {i+1}: {claim}" for i, claim in enumerate(batch)])
            
            try:
                system_prompt = """You are an expert fact checker who verifies if claims are supported by provided context.
For each claim, determine if it's supported by the context and provide the specific evidence.
Look for exact matches or paraphrases that convey the same factual information.
                
Return results as a JSON object where each claim is a key, and the value is an object with:
- "supported": boolean (true if claim is supported by context)
- "evidence": string (the specific part of the context that supports the claim)
- "source": string (the source identifier from the context)
- "confidence": float (0-1 indicating confidence in the grounding)"""

                # Include claim grounding information in the prompt
                grounding_summary = f"CLAIM GROUNDING RESULTS:\n"
                grounding_summary += f"- Total claims extracted: {len(claims)}\n"
                grounding_summary += f"- Grounded claims: {len(grounded_claims)}\n"
                grounding_summary += f"- Ungrounded claims: {len(ungrounded_claims)}\n\n"
                
                if ungrounded_claims:
                    grounding_summary += "UNGROUNDED CLAIMS:\n"
                    for claim in ungrounded_claims[:5]:  # Show up to 5 ungrounded claims
                        grounding_summary += f"- {claim}\n"
                    if len(ungrounded_claims) > 5:
                        grounding_summary += f"- ...and {len(ungrounded_claims) - 5} more\n"
                
                user_prompt = f"""CLAIMS TO GROUND:
{batch_text}

{grounding_summary}

CONTEXT:
{context}

GROUNDING RESULTS (as JSON):"""

                completion = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0
                )
                
                grounding_text = completion.choices[0].message.content
                
                # Extract JSON from the response
                json_match = re.search(r'\{.*\}', grounding_text, re.DOTALL)
                if json_match:
                    batch_results = json.loads(json_match.group(0))
                    
                    # Process results
                    for i, claim in enumerate(batch):
                        # Find the correct key in results (might be prefixed with "Claim X: ")
                        result_key = None
                        for key in batch_results:
                            if claim in key or (f"Claim {i+1}" in key):
                                result_key = key
                                break
                        
                        if result_key and result_key in batch_results:
                            result = batch_results[result_key]
                            grounding_results[claim] = result
                            
                            if result.get("supported", False):
                                grounded_claims.append({
                                    "claim": claim,
                                    "evidence": result.get("evidence", ""),
                                    "source": result.get("source", ""),
                                    "confidence": result.get("confidence", 0.5)
                                })
                            else:
                                ungrounded_claims.append(claim)
                        else:
                            # If claim not found in results
                            ungrounded_claims.append(claim)
                else:
                    # If JSON not found, consider all claims in this batch ungrounded
                    ungrounded_claims.extend(batch)
                    
            except Exception as e:
                print(f"Error grounding claims batch: {str(e)}")
                ungrounded_claims.extend(batch)
        
        return {
            "grounded_claims": grounded_claims,
            "ungrounded_claims": ungrounded_claims,
            "grounding_results": grounding_results
        }
    
    def _generate_correction(self, statement: str, context: str) -> str:
        """
        Generate a factual correction for a hallucinated statement based on available context.
        
        Args:
            statement: The hallucinated statement to correct
            context: The context information to use for correction
            
        Returns:
            A factual correction or clarification based on the available context
        """
        try:
            system_prompt = """You are an expert fact-checker tasked with correcting hallucinated statements in AI-generated content.
            
Given a potentially hallucinated statement and reference context:
1. Identify what facts in the statement are incorrect or unsupported
2. Provide a factual correction based ONLY on the information in the context
3. If no relevant information is found in the context, clearly state that

Your correction must:
- Be concise and focused only on addressing the factual inaccuracies
- Only use facts explicitly stated in the context
- Never introduce new information not found in the context
- Clearly indicate when information is unavailable in the context"""

            user_prompt = f"""POTENTIALLY HALLUCINATED STATEMENT:
{statement}

REFERENCE CONTEXT:
{context}

Provide a factual correction:"""

            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0
            )
            
            correction = completion.choices[0].message.content.strip()
            return correction
            
        except Exception as e:
            print(f"Error generating correction: {str(e)}")
            return "Unable to generate correction due to processing error. Please verify with authoritative sources."
    
    def evaluate_response(self, 
                         query: str, 
                         response: str, 
                         web_search_results: str,
                         sources: List[str]) -> Dict[str, Any]:
        """
        Evaluate a response for hallucinations using enhanced detection capabilities.
        
        Args:
            query: The user's query
            response: The generated response
            web_search_results: Raw web search results
            sources: List of source URLs
            
        Returns:
            Dictionary containing enhanced evaluation scores and hallucination assessment
        """
        print(f"Evaluating response for query: {query[:50]}...")
        
        # Generate cache key to avoid redundant evaluations - use full text hash
        cache_key = hashlib.md5((query + response + str(sources)).encode()).hexdigest()
        if cache_key in self.evaluation_cache:
            print("Using cached evaluation result")
            return self.evaluation_cache[cache_key]
        
        # 1. Prepare context with enhanced processing
        context_data = self.prepare_context(web_search_results, sources)
        context = context_data["processed_text"]
        
        # If no context available, we can't properly evaluate faithfulness
        if not context:
            print("WARNING: No context available to evaluate faithfulness")
            result = {
                "faithfulness": 0.0,
                "relevance": 0.0,
                "has_hallucination": True,
                "confidence": "Low",
                "explanation": "No context available to verify response",
                "hallucinated_statements": ["No source information available to verify any statements"],
                "warning": "WARNING: Unable to verify - No source information available",
                "sources": sources,
                "grounding": {"grounded_claims": [], "ungrounded_claims": []},
                "source_credibility": {}
            }
            self.evaluation_cache[cache_key] = result
            return result
        
        try:
            # 2. Verify source credibility
            source_credibility = self.verify_sources(sources)
            
            # 3. Extract claims from response
            claims = self.extract_claims(response)
            
            # 4. Ground claims to evidence in context
            grounding_info = self.ground_claims(claims, context_data)
            
            # 5. Determine overall hallucination metrics based on claim grounding
            grounded_claims = grounding_info["grounded_claims"]
            ungrounded_claims = grounding_info["ungrounded_claims"]
            
            # Calculate faithfulness based on proportion of grounded claims
            if claims:
                claims_faithfulness = len(grounded_claims) / len(claims)
            else:
                claims_faithfulness = 0.5  # Default if no claims detected
            
            # 6. Run full evaluation with enhanced prompt
            system_prompt = """You are an expert evaluator of AI-generated content, specializing in detecting hallucinations.
            
Your task is to evaluate if a response is factually accurate based on the provided context, sources, and claim grounding.

Critically evaluate the following aspects:
1. Faithfulness (0-1): How well the facts in the response are supported by the provided context
2. Relevance (0-1): How well the response addresses the user's query
3. Hallucination Detection: Identify any statements in the response that are not supported by the context

Pay special attention to:
- Claims that contradict the provided context
- Numerical values, dates, names, and specific technical details
- Subtle factual misrepresentations or exaggerations
- The credibility of the sources used

The response should be considered a hallucination if it:
- Makes specific factual claims not supported by the context
- Invents details not present in the context
- Contradicts information in the context
- Cites non-existent sources or misrepresents source content

Provide your evaluation as a structured JSON with these fields:
- faithfulness: float (0-1)
- relevance: float (0-1)
- has_hallucination: boolean
- confidence: "High" | "Medium" | "Low"
- explanation: string (detailed explanation of your evaluation)
- hallucinated_statements: array of strings (specific statements that appear to be hallucinated)

Note: I've already performed claim extraction and evidence grounding. Use this information in your evaluation, but conduct your own critical analysis as well."""

            # Include claim grounding information in the prompt
            grounding_summary = f"CLAIM GROUNDING RESULTS:\n"
            grounding_summary += f"- Total claims extracted: {len(claims)}\n"
            grounding_summary += f"- Grounded claims: {len(grounded_claims)}\n"
            grounding_summary += f"- Ungrounded claims: {len(ungrounded_claims)}\n\n"
            
            if ungrounded_claims:
                grounding_summary += "UNGROUNDED CLAIMS:\n"
                for claim in ungrounded_claims[:5]:  # Show up to 5 ungrounded claims
                    grounding_summary += f"- {claim}\n"
                if len(ungrounded_claims) > 5:
                    grounding_summary += f"- ...and {len(ungrounded_claims) - 5} more\n"
            
            user_prompt = f"""QUERY: {query}

RESPONSE TO EVALUATE:
{response}

{grounding_summary}

CONTEXT FROM SOURCES:
{context}

SOURCE CREDIBILITY:
{json.dumps(source_credibility, indent=2)}

Evaluate the response against the context. Identify any factual claims in the response that aren't supported by the context.
Return your evaluation in JSON format."""

            # Call OpenAI model for evaluation
            print(f"Calling {self.model} for hallucination evaluation...")
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0,
                response_format={"type": "json_object"}  # Request JSON format explicitly
            )
            
            evaluation_text = completion.choices[0].message.content
            print(f"Received evaluation response ({len(evaluation_text)} chars)")
            
            # Process evaluation result - handle JSON parsing more robustly
            try:
                evaluation_result = json.loads(evaluation_text)
            except json.JSONDecodeError as e:
                print(f"ERROR in hallucination evaluation: {str(e)}")
                
                # Try to extract JSON from the text if it's not properly formatted
                json_match = re.search(r'\{.*\}', evaluation_text, re.DOTALL)
                if json_match:
                    try:
                        evaluation_result = json.loads(json_match.group(0))
                    except:
                        # Fall back to default values if JSON can't be parsed
                        evaluation_result = {
                            "faithfulness": claims_faithfulness,
                            "relevance": 0.7,
                            "has_hallucination": len(ungrounded_claims) > 0,
                            "confidence": "Medium",
                            "explanation": "Failed to parse evaluation response",
                            "hallucinated_statements": ungrounded_claims
                        }
                else:
                    # Use default values if no JSON object found
                    evaluation_result = {
                        "faithfulness": claims_faithfulness,
                        "relevance": 0.7,
                        "has_hallucination": len(ungrounded_claims) > 0,
                        "confidence": "Medium",
                        "explanation": "Failed to extract evaluation results",
                        "hallucinated_statements": ungrounded_claims
                    }
            
            # Add grounding information
            evaluation_result["grounding"] = grounding_info
            evaluation_result["source_credibility"] = source_credibility
            
            # Add explicit problematic claims with corrections
            problematic_claims = []
            for statement in evaluation_result.get("hallucinated_statements", []):
                correction = self._generate_correction(statement, context)
                problematic_claims.append({
                    "text": statement,
                    "reason": "Not supported by sources",
                    "correction": correction
                })
            
            # Add faithfulness score based on our quantitative analysis
            if "faithfulness" not in evaluation_result:
                evaluation_result["faithfulness_score"] = claims_faithfulness
            else:
                evaluation_result["faithfulness_score"] = evaluation_result.pop("faithfulness")
                
            evaluation_result["problematic_claims"] = problematic_claims
            
            # Add overall assessment
            assessment = "VERIFIED: Information appears accurate."
            warning_level = "None"
            
            if evaluation_result.get("has_hallucination", False):
                if len(problematic_claims) > 3:
                    assessment = "MAJOR ISSUES: Multiple unsupported claims detected."
                    warning_level = "High"
                else:
                    assessment = "WARNING: CAUTION: Some information may not be supported by sources."
                    warning_level = "Medium"
                    
            evaluation_result["assessment"] = assessment
            evaluation_result["warning_level"] = warning_level
            
            # Cache the result
            self.evaluation_cache[cache_key] = evaluation_result
            
            print(f"Hallucination assessment: {assessment}")
            return evaluation_result
            
        except Exception as e:
            import traceback
            print(f"ERROR in hallucination evaluation: {str(e)}")
            traceback.print_exc()
            # Return default values in case of error
            return {
                "faithfulness": 0.0,
                "relevance": 0.0,
                "has_hallucination": True,
                "confidence": "Low",
                "explanation": f"Error evaluating response: {str(e)}",
                "hallucinated_statements": ["Unable to evaluate due to error"],
                "warning": "WARNING: ERROR: Unable to verify response due to technical issues",
                "sources": sources,
                "grounding": {"grounded_claims": [], "ungrounded_claims": []},
                "source_credibility": {}
            }
    
    def format_evaluation_results(self, evaluation: Dict[str, Any]) -> str:
        """
        Format evaluation results into a readable string for display.
        Enhanced with detailed grounding information.
        
        Args:
            evaluation: Dictionary containing evaluation results
            
        Returns:
            Formatted string with evaluation results
        """
        print("Formatting evaluation results for display")
        
        try:
            # Create header based on hallucination status
            if evaluation.get("has_hallucination", True):
                result = "\n\n**⚠️ Warning: Potential Hallucinations Detected**\n"
            else:
                result = "\n\n**✅ Verified: No Hallucinations Detected**\n"
            
            # Add main warning
            warning = evaluation.get('warning', 'No assessment available')
            result += f"{warning}\n\n"
            
            # Add scores
            result += f"**Evaluation Scores:**\n"
            result += f"- Faithfulness: {evaluation.get('faithfulness_score', 0):.2f}/1.00 ({evaluation.get('confidence', 'Low')} confidence)\n"
            result += f"- Relevance: {evaluation.get('relevance', 0):.2f}/1.00\n"
            
            # Add grounding statistics if available
            grounding = evaluation.get("grounding", {})
            if grounding:
                total_claims = grounding.get("total_claims", 0)
                grounded_claims = len(grounding.get("grounded_claims", []))
                
                if total_claims > 0:
                    result += f"\n**Claim Verification:**\n"
                    result += f"- {grounded_claims}/{total_claims} claims verified ({(grounded_claims/total_claims*100):.1f}%)\n"
            
            # Add hallucinated statements if any
            hallucinated_statements = evaluation.get("hallucinated_statements", [])
            if hallucinated_statements and hallucinated_statements[0] != "Unable to identify specific statements":
                result += f"\n**Potentially Unsupported Statements:**\n"
                # Limit to top 5 to avoid overwhelming the user
                for statement in hallucinated_statements[:5]:
                    result += f"- {statement}\n"
                
                if len(hallucinated_statements) > 5:
                    result += f"- ...and {len(hallucinated_statements) - 5} more statements\n"
            
            # Add source credibility if available
            source_credibility = evaluation.get("source_credibility", {})
            if source_credibility:
                result += f"\n**Source Credibility:**\n"
                # Display average credibility
                avg_credibility = sum(source_credibility.values()) / len(source_credibility) if source_credibility else 0
                result += f"- Average source credibility: {avg_credibility:.2f}/1.00\n"
            
            # Add sources
            sources = evaluation.get("sources", [])
            if sources:
                result += f"\n**Sources Checked:**\n"
                for i, source in enumerate(sources, 1):
                    cred = source_credibility.get(source, "N/A")
                    if isinstance(cred, float):
                        cred = f"{cred:.2f}"
                    result += f"{i}. {source} (Credibility: {cred})\n"
            
            # Add explanation
            explanation = evaluation.get("explanation", "")
            if explanation:
                result += f"\n**Detailed Analysis:**\n{explanation}\n"
            
            print(f"Formatted results: {result[:100]}...")
            return result
        except Exception as e:
            import traceback
            print(f"Error formatting evaluation results: {str(e)}")
            traceback.print_exc()
            return f"Error performing hallucination check: {str(e)}"
