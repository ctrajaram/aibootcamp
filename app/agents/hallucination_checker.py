"""
Hallucination Checker module for RAG applications.
This module provides functionality to evaluate the faithfulness of responses
generated by the Research Expert agent using OpenAI's evaluation capabilities.
"""

import os
from typing import List, Dict, Any, Optional
import re
from dotenv import load_dotenv
import json

# Load environment variables
load_dotenv()

# OpenAI imports
from openai import OpenAI

class HallucinationChecker:
    """
    A class to check for hallucinations in RAG responses using OpenAI's evaluation capabilities.
    Implements industry best practices for hallucination detection in AI-generated content.
    """
    
    def __init__(self, model: str = "gpt-4o"):
        """
        Initialize the HallucinationChecker with necessary models.
        
        Args:
            model: The OpenAI model to use for evaluation (default: gpt-4o)
        """
        print("Initializing HallucinationChecker...")
        
        # Initialize OpenAI client
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")
        
        self.client = OpenAI(api_key=openai_api_key)
        self.model = model
        print(f"HallucinationChecker initialized with model: {model}")
    
    def prepare_context(self, web_search_results: str, sources: List[str]) -> str:
        """
        Prepare context from web search results and sources for evaluation.
        
        Args:
            web_search_results: Raw web search results text
            sources: List of source URLs
            
        Returns:
            Processed context for evaluation
        """
        print(f"Preparing context from {len(sources)} sources")
        
        # If no web search results, return empty context
        if not web_search_results:
            print("No web search results provided")
            return ""
        
        # Extract relevant sections from web search results
        context_sections = []
        
        # Process each source URL to find related content in web search results
        for url in sources:
            # Clean URL for pattern matching
            clean_url = re.escape(url.strip())
            
            # Try to find content associated with this URL
            url_pattern = re.compile(f"{clean_url}[^h]*?(?=http|$)", re.DOTALL)
            matches = url_pattern.findall(web_search_results)
            
            if matches:
                for match in matches:
                    # Add URL and its associated content
                    section = f"SOURCE: {url}\nCONTENT: {match.strip()}"
                    context_sections.append(section)
        
        # If no specific sections were found, use the entire web search results
        if not context_sections:
            print("Using entire web search results as context")
            context_sections = [web_search_results]
        
        # Combine all context sections
        combined_context = "\n\n---\n\n".join(context_sections)
        
        # Truncate if too long (OpenAI has context limits)
        max_context_length = 12000  # Conservative limit
        if len(combined_context) > max_context_length:
            print(f"Context too long ({len(combined_context)} chars), truncating to {max_context_length}")
            combined_context = combined_context[:max_context_length] + "..."
        
        return combined_context
    
    def evaluate_response(self, 
                         query: str, 
                         response: str, 
                         web_search_results: str,
                         sources: List[str]) -> Dict[str, Any]:
        """
        Evaluate a response for hallucinations using OpenAI's evaluation capabilities.
        
        Args:
            query: The user's query
            response: The generated response
            web_search_results: Raw web search results
            sources: List of source URLs
            
        Returns:
            Dictionary containing evaluation scores and hallucination assessment
        """
        print(f"Evaluating response for query: {query[:50]}...")
        
        # Prepare context from web search results and sources
        context = self.prepare_context(web_search_results, sources)
        
        # If no context available, we can't properly evaluate faithfulness
        if not context:
            print("WARNING: No context available to evaluate faithfulness")
            return {
                "faithfulness": 0.0,
                "relevance": 0.0,
                "has_hallucination": True,
                "confidence": "Low",
                "explanation": "No context available to verify response",
                "warning": "WARNING: Unable to verify - No source information available",
                "sources": sources  # Include sources in the output
            }
        
        try:
            # Create evaluation prompt using industry best practices
            system_prompt = """You are an expert evaluator of AI-generated content, specializing in detecting hallucinations.
            
Your task is to evaluate if a response is factually accurate based on the provided context and sources.

Evaluate the response on these dimensions:
1. Faithfulness (0-1): How well the facts in the response are supported by the provided context
2. Relevance (0-1): How well the response addresses the user's query
3. Hallucination Detection: Identify any statements in the response that are not supported by the context

Provide your evaluation as a structured JSON with these fields:
- faithfulness: float (0-1)
- relevance: float (0-1)
- has_hallucination: boolean
- confidence: "High" | "Medium" | "Low"
- explanation: string (detailed explanation of your evaluation)
- hallucinated_statements: array of strings (specific statements that appear to be hallucinated)

Focus on factual accuracy rather than style or tone. A response with high faithfulness must have its key claims supported by the context."""

            user_prompt = f"""QUERY: {query}

RESPONSE TO EVALUATE:
{response}

CONTEXT FROM SOURCES:
{context}

Evaluate the response against the context. Identify any factual claims in the response that aren't supported by the context.
Return your evaluation in JSON format."""

            # Call OpenAI for evaluation
            print(f"Calling {self.model} for hallucination evaluation...")
            completion = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0
            )
            
            # Extract the evaluation response
            evaluation_text = completion.choices[0].message.content
            print(f"Received evaluation response ({len(evaluation_text)} chars)")
            
            # Parse JSON from the response
            try:
                # Extract JSON from the response text
                json_match = re.search(r'\{.*\}', evaluation_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                    evaluation = json.loads(json_str)
                else:
                    # Fallback if no JSON is found
                    print("No JSON found in evaluation response, using default values")
                    evaluation = {
                        "faithfulness": 0.5,
                        "relevance": 0.5,
                        "has_hallucination": True,
                        "confidence": "Medium",
                        "explanation": "Could not parse evaluation properly",
                        "hallucinated_statements": ["Unable to identify specific statements"]
                    }
            except json.JSONDecodeError as e:
                print(f"Failed to parse JSON from response: {e}")
                evaluation = {
                    "faithfulness": 0.5,
                    "relevance": 0.5,
                    "has_hallucination": True,
                    "confidence": "Medium",
                    "explanation": "Could not parse evaluation properly",
                    "hallucinated_statements": ["Unable to identify specific statements"]
                }
            
            # Generate appropriate warning message
            if evaluation.get("has_hallucination", True):
                faithfulness = evaluation.get("faithfulness", 0)
                if faithfulness < 0.3:
                    warning = "WARNING: HIGH RISK: Significant hallucinations detected. Verify information independently."
                elif faithfulness < 0.7:
                    warning = "WARNING: CAUTION: Some information may not be supported by sources."
                else:
                    warning = "WARNING: MINOR: Minor discrepancies detected, but most information is supported."
            else:
                warning = "VERIFIED: Response appears to be well-grounded in the provided sources."
            
            # Prepare final evaluation result
            result = {
                "faithfulness": round(evaluation.get("faithfulness", 0), 2),
                "relevance": round(evaluation.get("relevance", 0), 2),
                "has_hallucination": evaluation.get("has_hallucination", True),
                "confidence": evaluation.get("confidence", "Medium"),
                "explanation": evaluation.get("explanation", "No explanation provided"),
                "hallucinated_statements": evaluation.get("hallucinated_statements", []),
                "warning": warning,
                "sources": sources  # Include sources in the output
            }
            
            print(f"Hallucination assessment: {warning}")
            return result
            
        except Exception as e:
            import traceback
            print(f"ERROR in hallucination evaluation: {str(e)}")
            traceback.print_exc()
            # Return default values in case of error
            return {
                "faithfulness": 0.0,
                "relevance": 0.0,
                "has_hallucination": True,
                "confidence": "Low",
                "explanation": f"Error evaluating response: {str(e)}",
                "hallucinated_statements": ["Unable to evaluate due to error"],
                "warning": "WARNING: ERROR: Unable to verify response due to technical issues",
                "sources": sources  # Include sources in the output
            }
    
    def format_evaluation_results(self, evaluation: Dict[str, Any]) -> str:
        """
        Format evaluation results into a readable string for display.
        
        Args:
            evaluation: Dictionary containing evaluation results
            
        Returns:
            Formatted string with evaluation results
        """
        print("Formatting evaluation results for display")
        
        try:
            # Create header based on hallucination status
            if evaluation.get("has_hallucination", True):
                result = "\n\n**Warning Hallucination Check:**\n"
            else:
                result = "\n\n**Verified Hallucination Check:**\n"
            
            # Add main warning - replace special characters with ASCII alternatives
            warning = evaluation.get('warning', 'No assessment available')
            # Replace special characters with ASCII alternatives
            warning = warning.replace("⚠️", "WARNING:")
            warning = warning.replace("✅", "VERIFIED:")
            result += f"{warning}\n\n"
            
            # Add scores
            result += f"**Evaluation Scores:**\n"
            result += f"- Faithfulness: {evaluation.get('faithfulness', 0):.2f}/1.00 ({evaluation.get('confidence', 'Low')} confidence)\n"
            result += f"- Relevance: {evaluation.get('relevance', 0):.2f}/1.00\n"
            
            # Add hallucinated statements if any
            hallucinated_statements = evaluation.get("hallucinated_statements", [])
            if hallucinated_statements and len(hallucinated_statements) > 0 and hallucinated_statements[0] != "Unable to identify specific statements":
                result += f"\n**Potentially Unsupported Statements:**\n"
                for statement in hallucinated_statements:  # Show all statements
                    result += f"- {statement}\n"
            
            # Add sources
            sources = evaluation.get("sources", [])
            if sources:
                result += f"\n**Sources Checked:**\n"
                for i, source in enumerate(sources, 1):
                    result += f"{i}. {source}\n"
            
            print(f"Formatted results: {result[:100]}...")
            return result
        except Exception as e:
            import traceback
            print(f"Error formatting evaluation results: {str(e)}")
            traceback.print_exc()
            return f"Error performing hallucination check: {str(e)}"
